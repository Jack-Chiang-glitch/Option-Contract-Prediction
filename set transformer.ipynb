{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35210eb-ba87-4db7-ab84-79403bd2f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import sqlite3\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce70f24-ef60-4fad-b111-a95ce86a66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calendar:\n",
    "    def __init__(self, start_date, end_date):\n",
    "        # å–å¾—ç¾è‚¡ï¼ˆNYSEï¼‰äº¤æ˜“æ—¥æ›†\n",
    "        nyse = mcal.get_calendar('NYSE')\n",
    "        # ç²å–è©²æ™‚é–“ç¯„åœå…§çš„äº¤æ˜“æ—¥\n",
    "        schedule = nyse.valid_days(start_date=start_date, end_date=end_date)\n",
    "        # è½‰ç‚º DataFrame\n",
    "        trading_days = schedule.tz_convert(None).date\n",
    "        self.trading_days = [str(trading_day) \n",
    "                             for trading_day in trading_days]\n",
    "\n",
    "    def count_trading_days(self, start, end):\n",
    "        # è¨ˆç®— start ~ end ä¹‹é–“çš„äº¤æ˜“æ—¥æ•¸é‡\n",
    "        return sum(1 for day in pd.date_range(start, end) if day.date() in self.trading_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e127967a-77eb-4c9c-9bda-4cc9db138168",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptionQuery:\n",
    "    def __init__(self,ticker):\n",
    "        self.price = yf.download(ticker, start='2010-01-01', end='2025-02-01', progress=False)\n",
    "        self.ticker = ticker\n",
    "        \n",
    "    def _get_close_price(self, DATE):\n",
    "        return self.price.loc[DATE]['Close']\n",
    "\n",
    "    def _get_raw_option_df(self, DATE):\n",
    "        with sqlite3.connect(\"options_data.db\") as conn:\n",
    "            cursor = conn.cursor()\n",
    "        \n",
    "            # åŸ·è¡ŒæŸ¥è©¢\n",
    "            cursor.execute(\"SELECT * FROM options_data WHERE date = ?\", (DATE,))\n",
    "            \n",
    "            # å–å¾—æ‰€æœ‰ç¬¦åˆæ¢ä»¶çš„è³‡æ–™\n",
    "            rows = cursor.fetchall()\n",
    "            # å–å¾—æ¬„ä½åç¨±\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        # è½‰æˆ DataFrame æ–¹ä¾¿é¡¯ç¤º\n",
    "        return pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    def _get_normalized_option_data(self, DATE):\n",
    "   \n",
    "        \n",
    "        df = self._get_raw_option_df(DATE)\n",
    "        \n",
    "\n",
    "        \n",
    "        if df.empty:\n",
    "            return None\n",
    "            \n",
    "        # è¨ˆç®—å‰©é¤˜å¤©æ•¸\n",
    "        \n",
    "        df = df[df['volume']!=0]\n",
    "\n",
    "        if df.empty:\n",
    "            return None\n",
    "        \n",
    "        df[\"expiration\"] = pd.to_datetime(df[\"expiration\"])\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        # è¨ˆç®—äº¤æ˜“æ—¥æ•¸\n",
    "        df[\"days_to_expiration\"] = (df[\"expiration\"] - df[\"date\"]).dt.days / 365\n",
    "        # åˆªé™¤ä¸éœ€è¦æ¬„ä½\n",
    "        df = df.drop(columns=[\"contractID\", \"date\", \"expiration\", \"symbol\"])\n",
    "\n",
    "        \n",
    "        \n",
    "        df = pd.get_dummies(df, columns=[\"type\"])#.astype(int)\n",
    "        df.columns = df.columns.str.lower()\n",
    "        \n",
    "        \n",
    "        df[[\"type_call\", \"type_put\"]] = df[[\"type_call\", \"type_put\"]].astype(int)\n",
    "        #df[[\"type_CALL\", \"type_PUT\"]] = df[[\"type_Call\", \"type_Put\"]].astype(int)\n",
    "\n",
    "        \n",
    "        \n",
    "        df['open_interest'] =  df['open_interest'] / df['open_interest'].sum() *100\n",
    "        df['bid_size'] = df['bid_size'] / df['bid_size'].sum() * 100\n",
    "        df['ask_size'] = df['ask_size'] / df['ask_size'].sum() * 100\n",
    "        \n",
    "        df['volume'] = df['volume'] / df['volume'].sum() * 100\n",
    "        df[['last', 'mark', 'bid', 'ask']] = df[['last', 'mark', 'bid', 'ask']].div(df['strike'], axis=0)\n",
    "        close_price = self._get_close_price(DATE)\n",
    "        df['strike'] =  df['strike'] / close_price\n",
    "        \n",
    "        #print(df)\n",
    "\n",
    "        # âœ… æª¢æŸ¥æ˜¯å¦æœ‰ NaN å€¼\n",
    "        if df.isna().any().any():\n",
    "            return None\n",
    "\n",
    "        tensor = self.Z_score(torch.tensor(df.values))\n",
    "        #print(pd.DataFrame(tensor))\n",
    "        return tensor\n",
    "\n",
    "    def Z_score(self, tensor, execute=True):\n",
    "        if not execute:\n",
    "            return tensor\n",
    "        else:\n",
    "            # ğŸ”¹ åªæ¨™æº–åŒ–å‰ 15 åˆ—ï¼ˆç¬¬ 0~14 åˆ—ï¼‰\n",
    "            cols_to_normalize = list(range(15))  # é¸æ“‡å‰ 15 åˆ—\n",
    "            mean = tensor[:, cols_to_normalize].mean(dim=0, keepdim=True)  # è¨ˆç®—åˆ—å‡å€¼\n",
    "            std = tensor[:, cols_to_normalize].std(dim=0, keepdim=True)  # è¨ˆç®—åˆ—æ¨™æº–å·®\n",
    "            \n",
    "            # **é¿å…é™¤ä»¥ 0ï¼Œç¢ºä¿æ•¸å€¼ç©©å®š**\n",
    "            std[std == 0] = 1e-8\n",
    "            \n",
    "            # ğŸ”¹ æ¨™æº–åŒ–å‰ 15 åˆ—ï¼Œä¸å½±éŸ¿å…¶ä»–åˆ—\n",
    "            tensor[:, cols_to_normalize] = (tensor[:, cols_to_normalize] - mean) / std\n",
    "            return tensor\n",
    "            \n",
    "    \n",
    "        \n",
    "\n",
    "    def option_data(self, DATE):\n",
    "        return self._get_normalized_option_data(DATE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8fe5be2-3435-45f7-9004-1a3569a03970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5654,  2.3158,  2.2752,  ...,  0.0082,  1.0000,  0.0000],\n",
       "        [-1.1101,  1.3082,  1.2321,  ...,  0.0082,  1.0000,  0.0000],\n",
       "        [-1.1101, -0.5697, -0.5682,  ...,  0.0082,  0.0000,  1.0000],\n",
       "        ...,\n",
       "        [ 2.7598, -0.3952, -0.3987,  ...,  2.0411,  1.0000,  0.0000],\n",
       "        [ 2.9874, -0.4149, -0.4221,  ...,  2.0411,  1.0000,  0.0000],\n",
       "        [ 3.2150, -0.4445, -0.4404,  ...,  2.0411,  1.0000,  0.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = OptionQuery('AAPL')\n",
    "e = query.option_data('2015-01-06')\n",
    "\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9778dc5-d95e-408d-985f-339770880b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba165cc7-b89f-4a3e-9076-d96787e47196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "514d7c75-8ba6-43cb-b5a3-c8c4d1959164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptionDataset:\n",
    "    \"\"\"\n",
    "    é€é `OptionQuery` å‹•æ…‹åŠ è¼‰é¸æ“‡æ¬Šæ•¸æ“š\n",
    "    \"\"\"\n",
    "    def __init__(self, ticker, start_date, end_date):\n",
    "        self.query = OptionQuery(ticker)\n",
    "        self.trading_days = Calendar(start_date, end_date).trading_days\n",
    "        self.historical_return = self._get_historical_option_return(start_date, end_date)\n",
    "\n",
    "        #print(len(self.trading_days))\n",
    "        #print(len(self.historical_return))\n",
    "\n",
    "    def _get_historical_option_return(self, start, end):\n",
    "        historical_data = yf.download('AAPL', start='2014-01-01', end='2025-02-01', progress=False)\n",
    "        historical_data[\"daily_return\"] = historical_data[\"Adj Close\"].pct_change()\n",
    "        \n",
    "        historical_return = historical_data['daily_return']\n",
    "        historical_return = historical_return.shift(-1)[start : end]\n",
    "        return historical_return\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.trading_days)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        date = self.trading_days[idx]\n",
    "        data = self.query.option_data(date)\n",
    "\n",
    "        if data is None:\n",
    "            data = torch.zeros((1, 18))\n",
    "\n",
    "        feature = data.to(torch.float32)\n",
    "        label = torch.tensor(self.historical_return.loc[date], dtype=torch.float32)\n",
    "        return feature, label\n",
    "#-----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9723b89-e6e3-496c-9b29-ca2ba98cc633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹è¨“ç·´é›† (TRAIN) å’Œ æ¸¬è©¦é›† (TEST)\n",
    "train_dataset = OptionDataset('AAPL', \"2015-01-01\", \"2015-12-31\")\n",
    "test_dataset = OptionDataset('AAPL', \"2022-01-01\", \"2022-12-31\")\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    è‡ªå®šç¾© collate_fnï¼Œå¡«å……æ™‚éš¨æ©ŸæŒ‘é¸ä¸é‡è¤‡çš„åŸå§‹å…ƒç´ ä¾†è£œé½Šé•·åº¦\n",
    "    åƒæ•¸ï¼š\n",
    "      batch: list, æ¯å€‹å…ƒç´ æ˜¯ä¸€å€‹ tuple (feature, label)\n",
    "             å…¶ä¸­ feature çš„å½¢ç‹€ç‚º [Ni, d_dim]ï¼Œlabel ç‚º scalar tensor\n",
    "    è¿”å›ï¼š\n",
    "      padded_features: tensor, å½¢ç‹€ç‚º [batch_size, N_max, d_dim]\n",
    "      labels: tensor, å½¢ç‹€ç‚º [batch_size]\n",
    "    \"\"\"\n",
    "    # è§£æ batch ä¸­çš„ features å’Œ labels\n",
    "    features, labels = zip(*batch)\n",
    "    \n",
    "    # æ‰¾åˆ° batch å…§æœ€å¤§åºåˆ—é•·åº¦ (N_max)\n",
    "    N_max = max(f.shape[0] for f in features)\n",
    "\n",
    "    # è‡ªå®šç¾©å¡«å……ï¼šéš¨æ©Ÿå¾åŸæ•¸æ“šæŒ‘é¸ä¸é‡è¤‡çš„å€¼ä¾†å¡«å……\n",
    "    def pad_with_random_selection(tensor, target_length):\n",
    "        current_length = tensor.shape[0]\n",
    "        if current_length == target_length:\n",
    "            return tensor  # é•·åº¦å·²ç¶“å¤ äº†ï¼Œä¸éœ€è¦å¡«å……\n",
    "        \n",
    "        # å–å¾—åŸå§‹æ•¸æ“š\n",
    "        original_data = tensor.tolist()\n",
    "        while len(original_data) < target_length:\n",
    "            # éš¨æ©Ÿé¸å–åŸå§‹æ•¸æ“šä¸­çš„å€¼ï¼Œç¢ºä¿ä¸é‡è¤‡\n",
    "            sampled_values = random.sample(tensor.tolist(), min(len(tensor), target_length - len(original_data)))\n",
    "            original_data.extend(sampled_values)\n",
    "        \n",
    "        # è½‰å› tensor\n",
    "        return torch.tensor(original_data, dtype=tensor.dtype, device=tensor.device)\n",
    "\n",
    "    # å°æ‰€æœ‰ features é€²è¡Œå¡«å……\n",
    "    padded_features = [pad_with_random_selection(f, N_max) for f in features]\n",
    "    \n",
    "    # å †ç–Šæˆ batch tensor\n",
    "    padded_features = torch.stack(padded_features, dim=0)  # [batch_size, N_max, d_dim]\n",
    "\n",
    "    # è½‰æ› labels ç‚º tensor\n",
    "    labels = torch.stack(labels, dim=0)  # [batch_size]\n",
    "\n",
    "    return padded_features, labels*100\n",
    "\n",
    "\n",
    "#def my_collate_fn(batch):\n",
    "#    \"\"\"\n",
    "#    åƒæ•¸ï¼š\n",
    "#      batch: list, æ¯å€‹å…ƒç´ æ˜¯ä¸€å€‹ tuple (feature, label)\n",
    "#             å…¶ä¸­ feature çš„å½¢ç‹€ç‚º [Ni, d_dim]ï¼Œlabel ç‚º scalar tensor\n",
    "#    è¿”å›ï¼š\n",
    "#      padded_features: tensor, å½¢ç‹€ç‚º [batch_size, N_max, d_dim]\n",
    "#      labels: tensor, å½¢ç‹€ç‚º [batch_size]\n",
    "#    \"\"\"\n",
    "#    # å°‡ batch ä¸­çš„ feature èˆ‡ label åˆ†é›¢\n",
    "#    features, labels = zip(*batch)\n",
    "#    \n",
    "#    # å° feature é€²è¡Œå¡«å…… (batch_first=True è¡¨ç¤ºè¼¸å‡ºå½¢ç‹€ç‚º [batch, seq_len, feature_dim])\n",
    "#    padded_features = pad_sequence(features, batch_first=True, padding_value=0)\n",
    "#    \n",
    "#    # å°‡ label çµ„æˆ tensor\n",
    "#    labels = torch.stack(labels, dim=0)\n",
    "#    \n",
    "#    return padded_features, labels\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=40, shuffle=True, collate_fn=my_collate_fn)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=40, shuffle=True, collate_fn=my_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638aa2e8-54c4-4476-a9ea-03b720a6007b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "846e249e-0de2-48b4-86d4-8d1c234868a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Set Transformer æ¨¡çµ„ (ç„¡ Mask ç‰ˆæœ¬)\n",
    "# ---------------------------\n",
    "\n",
    "class MAB(nn.Module):\n",
    "    \"\"\"\n",
    "    MAB(Q, K) = Multi-Head Attention Block\n",
    "    é€™è£¡ä¸å¯¦ä½œ maskï¼Œå› ç‚ºä¸€æ¬¡åªè™•ç†ä¸€å€‹ dayï¼Œä¸éœ€è¦ paddingã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_Q, dim_K, dim_out, num_heads, ln=True):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_out = dim_out\n",
    "\n",
    "        self.W_Q = nn.Linear(dim_Q, dim_out)\n",
    "        self.W_K = nn.Linear(dim_K, dim_out)\n",
    "        self.W_V = nn.Linear(dim_K, dim_out)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim_out, dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_out, dim_out),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(dim_out) if ln else nn.Identity()\n",
    "        self.ln2 = nn.LayerNorm(dim_out) if ln else nn.Identity()\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        \"\"\"\n",
    "        Q, K shape = (1, Nq, d_in), (1, Nk, d_in)\n",
    "        é€™è£¡å› ç‚ºä¸€æ¬¡ä¸€å¤©ï¼Œæ‰€ä»¥ batch_size=1\n",
    "        ä½†ä¹Ÿå¯ä»¥å¯«æˆ (batch, Nq, d_in) (batch, Nk, d_in) åªæ˜¯ batch=1\n",
    "        \"\"\"\n",
    "        B, Nq, _ = Q.shape\n",
    "        _, Nk, _ = K.shape\n",
    "\n",
    "        # ç·šæ€§æŠ•å½±\n",
    "        Q_ = self.W_Q(Q)  # (1, Nq, dim_out)\n",
    "        K_ = self.W_K(K)  # (1, Nk, dim_out)\n",
    "        V_ = self.W_V(K)  # (1, Nk, dim_out)\n",
    "\n",
    "        d = self.dim_out\n",
    "        d_head = d // self.num_heads\n",
    "        # æ‹†å¤šé ­\n",
    "        Q_ = Q_.view(B, Nq, self.num_heads, d_head).transpose(1, 2)  # (1, h, Nq, d_head)\n",
    "        K_ = K_.view(B, Nk, self.num_heads, d_head).transpose(1, 2)\n",
    "        V_ = V_.view(B, Nk, self.num_heads, d_head).transpose(1, 2)\n",
    "\n",
    "        # æ³¨æ„åŠ› scores: (1, h, Nq, Nk)\n",
    "        scores = torch.matmul(Q_, K_.transpose(-2, -1)) / (d_head**0.5)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        H = torch.matmul(attn, V_)  # (1, h, Nq, d_head)\n",
    "\n",
    "        # æ‹¼å› (1, Nq, dim_out)\n",
    "        H = H.transpose(1, 2).contiguous().view(B, Nq, d)\n",
    "\n",
    "        # æ®˜å·® + LayerNorm\n",
    "        H = self.ln1(H + self.W_Q(Q))\n",
    "        # å‰é¥‹\n",
    "        H2 = self.fc(H)\n",
    "        H = self.ln2(H + H2)\n",
    "        return H\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    \"\"\" Self-Attention Block: SAB(X) = MAB(X, X) \"\"\"\n",
    "    def __init__(self, dim_in, dim_out, num_heads=4, ln=True):\n",
    "        super().__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads=num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X shape = (1, N, dim_in)\n",
    "        return self.mab(X, X)\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    \"\"\"\n",
    "    Pooling by Multihead Attention:\n",
    "    PMA(S, X) = MAB(S, X)\n",
    "    num_seeds=1 -> å–å¾—æ•´å€‹é›†åˆçš„ä¸€å€‹å‘é‡è¡¨ç¤º\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, num_heads=4, num_seeds=1, ln=True):\n",
    "        super().__init__()\n",
    "        self.num_seeds = num_seeds\n",
    "        self.dim_in = dim_in\n",
    "        self.S = nn.Parameter(torch.Tensor(num_seeds, dim_in))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim_in, dim_in, dim_in, num_heads=num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X shape = (1, N, dim_in)\n",
    "        # S shape = (num_seeds, dim_in) -> (1, num_seeds, dim_in)\n",
    "        B = X.shape[0]  # ä¸€èˆ¬æƒ…æ³ batch=1\n",
    "        S = self.S.unsqueeze(0).expand(B, self.num_seeds, self.dim_in)\n",
    "        H = self.mab(S, X)  # (1, num_seeds, dim_in)\n",
    "        return H\n",
    "\n",
    "class SetTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    çµåˆ SAB + PMAï¼Œæœ€å¾Œç”¨ Linear åšå›æ­¸\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_input=7, hidden_dim=128, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.sab1 = SAB(dim_input, hidden_dim, num_heads=num_heads, ln=True)\n",
    "        self.sab2 = SAB(hidden_dim, hidden_dim, num_heads=num_heads, ln=True)\n",
    "        self.pma = PMA(hidden_dim, num_heads=num_heads, num_seeds=1, ln=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 10)  # è¼¸å‡º 1 ç¶­(å›æ­¸)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X shape = (1, N, dim_input)\n",
    "        \"\"\"\n",
    "        H = self.sab1(X)        # (1, N, hidden_dim)\n",
    "\n",
    "        H = self.sab2(H)        # (1, N, hidden_dim)\n",
    "        H = self.pma(H)         # (1, 1, hidden_dim)\n",
    "        H = H.squeeze(1)        # (1, hidden_dim)\n",
    "        out = self.fc(H)        # (1, 1)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8015be2b-6e2a-4656-9a6d-28bc4ffd1740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 30])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(18,80)\n",
    "        self.fc2 = nn.Linear(80,160)\n",
    "        self.fc3 = nn.Linear(160,200)\n",
    "        self.fc4 = nn.Linear(200,256)\n",
    "        \n",
    "        self.fc = nn.Linear(256,30)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU()\n",
    "\n",
    "    def Standardize(self, data):\n",
    "        # è¨ˆç®—æ¯å€‹ batch & æ¯å€‹ feature åœ¨ 200 å€‹åˆç´„ä¸Šçš„å‡å€¼ & æ¨™æº–å·®\n",
    "        mean = data.mean(dim=1, keepdim=True)  # (3, 1, 18)ï¼Œæ¯å€‹ batch çš„ feature-wise å‡å€¼\n",
    "        std =  data.std(dim=1, keepdim=True)    # (3, 1, 18)ï¼Œæ¯å€‹ batch çš„ feature-wise æ¨™æº–å·®\n",
    "        # æ¨™æº–åŒ–ï¼Œæ¯å€‹ feature åœ¨ 200 å€‹åˆç´„ä¸Šè®Šæˆ 0 å‡å€¼ã€1 æ¨™æº–å·®\n",
    "        normalized_output = (data - mean) / (std + 1e-6)  # é¿å…é™¤ä»¥ 0\n",
    "        return normalized_output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.LeakyReLU(self.Standardize(self.fc1(x)))\n",
    "        x = self.LeakyReLU(self.Standardize(self.fc2(x)))\n",
    "        x = self.LeakyReLU(self.Standardize(self.fc3(x)))\n",
    "        x = self.LeakyReLU(self.Standardize(self.fc4(x)))\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return torch.tanh(x)/10*100\n",
    "        \n",
    "Input = torch.randn((3,100,18))\n",
    "model = FC()\n",
    "print(model(Input).shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a53fc7a-6906-4619-b0f3-fde9d5e8eccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6208c5f3-2346-4482-93ca-88c283819e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "# å»ºç«‹æ¨¡å‹\n",
    "#model = SetTransformer(dim_input=18, hidden_dim=128, num_heads=4)\n",
    "model = FC()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "443dfe90-d377-48a1-bc0e-f8f44be82a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# åˆå§‹åŒ– Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "# ğŸ”¹ ä½¿ç”¨ `ReduceLROnPlateau`ï¼Œç•¶ `valid_loss` åœæ»¯ 5 å€‹ Epochï¼Œå­¸ç¿’ç‡ç¸®å° 0.5 å€\n",
    "#scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54c563-7e64-4d13-bf41-af92a10c646a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6427f6-dc6a-481c-8655-ee6b7a88236a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd2142dc-c61f-4896-a5f1-296e79ae646c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc : 0.47857141494750977\n",
      "Test acc : 0.5314935445785522\n",
      "Epoch 1, Train Loss: 4.4690, Valid Loss: 5.7339\n",
      "Train acc : 0.5035714507102966\n",
      "Test acc : 0.4743506610393524\n",
      "Epoch 2, Train Loss: 2.8277, Valid Loss: 4.8904\n",
      "Train acc : 0.5273810029029846\n",
      "Test acc : 0.4555194675922394\n",
      "Epoch 3, Train Loss: 2.8528, Valid Loss: 5.1838\n",
      "Train acc : 0.4928571581840515\n",
      "Test acc : 0.4707792103290558\n",
      "Epoch 4, Train Loss: 2.7881, Valid Loss: 5.1451\n",
      "Train acc : 0.5404762029647827\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 102\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;66;03m# **åˆ—å°è¨“ç·´ & é©—è­‰æå¤±**\u001b[39;00m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Valid Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_valid_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m train_single_day()\n",
      "Cell \u001b[0;32mIn[57], line 79\u001b[0m, in \u001b[0;36mtrain_single_day\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m correctness_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# **é—œé–‰æ¢¯åº¦ï¼Œé¿å…å½±éŸ¿æ¨¡å‹åƒæ•¸**\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m     80\u001b[0m         feats, label \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     81\u001b[0m         label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/finance/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/finance/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/finance/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[43], line 28\u001b[0m, in \u001b[0;36mOptionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     27\u001b[0m     date \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrading_days[idx]\n\u001b[0;32m---> 28\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery\u001b[38;5;241m.\u001b[39moption_data(date)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m         data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m18\u001b[39m))\n",
      "Cell \u001b[0;32mIn[3], line 98\u001b[0m, in \u001b[0;36mOptionQuery.option_data\u001b[0;34m(self, DATE)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moption_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, DATE):\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_normalized_option_data(DATE)\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mOptionQuery._get_normalized_option_data\u001b[0;34m(self, DATE)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_normalized_option_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, DATE):\n\u001b[0;32m---> 27\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_raw_option_df(DATE)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mOptionQuery._get_raw_option_df\u001b[0;34m(self, DATE)\u001b[0m\n\u001b[1;32m     19\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# è½‰æˆ DataFrame æ–¹ä¾¿é¡¯ç¤º\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows, columns\u001b[38;5;241m=\u001b[39mcolumns)\n",
      "File \u001b[0;32m~/.conda/envs/finance/lib/python3.12/site-packages/pandas/core/frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[1;32m    852\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[1;32m    853\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[1;32m    854\u001b[0m         data,\n\u001b[1;32m    855\u001b[0m         columns,\n\u001b[1;32m    856\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    857\u001b[0m         dtype,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    860\u001b[0m         arrays,\n\u001b[1;32m    861\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    865\u001b[0m     )\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/finance/lib/python3.12/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/finance/lib/python3.12/site-packages/pandas/core/internals/construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/.conda/envs/finance/lib/python3.12/site-packages/pandas/core/internals/construction.py:945\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m--> 945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m contents, columns\n",
      "File \u001b[0;32m~/.conda/envs/finance/lib/python3.12/site-packages/pandas/core/internals/construction.py:1070\u001b[0m, in \u001b[0;36mconvert_object_array\u001b[0;34m(content, dtype, dtype_backend, coerce_float)\u001b[0m\n\u001b[1;32m   1066\u001b[0m             arr \u001b[38;5;241m=\u001b[39m maybe_cast_to_datetime(arr, dtype)\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\n\u001b[0;32m-> 1070\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [convert(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m content]\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays\n",
      "File \u001b[0;32m~/.conda/envs/finance/lib/python3.12/site-packages/pandas/core/internals/construction.py:1030\u001b[0m, in \u001b[0;36mconvert_object_array.<locals>.convert\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(arr):\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1030\u001b[0m         arr \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(\n\u001b[1;32m   1031\u001b[0m             arr,\n\u001b[1;32m   1032\u001b[0m             try_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[1;32m   1033\u001b[0m             convert_to_nullable_dtype\u001b[38;5;241m=\u001b[39mdtype_backend \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1034\u001b[0m         )\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;66;03m# Notes on cases that get here 2023-02-15\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m         \u001b[38;5;66;03m# 1) we DO get here when arr is all Timestamps and dtype=None\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;66;03m# 2) disabling this doesn't break the world, so this must be\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m         \u001b[38;5;66;03m#    getting caught at a higher level\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;66;03m# 3) passing convert_non_numeric to maybe_convert_objects get this right\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;66;03m# 4) convert_non_numeric?\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32mlib.pyx:2543\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/finance/lib/python3.12/site-packages/numpy/_core/numeric.py:290\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order, device, like)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_full_dispatcher\u001b[39m(\n\u001b[1;32m    285\u001b[0m     shape, fill_value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, like\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    286\u001b[0m ):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(like,)\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;129m@finalize_array_function_like\u001b[39m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfull\u001b[39m(shape, fill_value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, like\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    Return a new array of given shape and type, filled with `fill_value`.\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    342\u001b[0m \n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m like \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def train_single_day():\n",
    "    for epoch in range(EPOCHS):\n",
    "        # **è¨“ç·´éšæ®µ**\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        count = 0\n",
    "        \n",
    "        correctness_list = []\n",
    "        for batch in train_loader:\n",
    "            count+=1\n",
    "            if count%20==0:\n",
    "                print(count)\n",
    "                #print(total_train_loss)\n",
    "            \n",
    "            feats, label = batch\n",
    "            #print(label.shape)\n",
    "            label = label.unsqueeze(1).repeat(1,30)\n",
    "            \n",
    "\n",
    "            \n",
    "            feats, label = feats.to(device), label.to(device)\n",
    "\n",
    "            \n",
    "                \n",
    "            \n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(feats)  \n",
    "\n",
    "            #print(preds.shape)\n",
    "            #print(label.shape)\n",
    "            #print(feats.shape)\n",
    "            #if torch.isnan(feats).any():\n",
    "            #    print(\"âš ï¸ Tensor å«æœ‰ NaN å€¼ï¼\")\n",
    "            loss = criterion(preds, label)\n",
    "            #if torch.isnan(loss):\n",
    "                #print(label)\n",
    "                #print(feats)\n",
    "            loss.backward()\n",
    "\n",
    "            \"\"\"\n",
    "            # ----------- æ‰“å°æ¢¯åº¦é•·åº¦ (Gradient Norm) -----------\n",
    "            total_norm = 0.0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    # è¨ˆç®—æ¯å€‹åƒæ•¸çš„äºŒç¯„æ•¸ (L2 norm)\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "            print(f\"ç›®å‰ batch çš„æ¢¯åº¦ç¯„æ•¸: {total_norm:.4f}\")\n",
    "            # -----------------------------------------------------\n",
    "            \"\"\"\n",
    "\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "\n",
    "            preds, label = preds.mean(dim=1), label.mean(dim=1)\n",
    "            correctness_list.append((preds*label>0).float().mean())\n",
    "            \n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = total_train_loss / count\n",
    "        \n",
    "        train_acc = np.array([t.cpu().numpy() for t in correctness_list]).mean()\n",
    "        print(f'Train acc : {train_acc}')\n",
    "\n",
    "        \n",
    "        # **é©—è­‰éšæ®µ**\n",
    "        model.eval()\n",
    "        total_valid_loss = 0.0\n",
    "\n",
    "        \n",
    "        correctness_list = []\n",
    "        with torch.no_grad():  # **é—œé–‰æ¢¯åº¦ï¼Œé¿å…å½±éŸ¿æ¨¡å‹åƒæ•¸**\n",
    "            for batch in test_loader:\n",
    "                feats, label = batch\n",
    "                label = label.unsqueeze(1).repeat(1,30)\n",
    "                \n",
    "                feats, label = feats.to(device), label.to(device)\n",
    "\n",
    "                preds = model(feats) \n",
    "                loss = criterion(preds, label)\n",
    "\n",
    "                preds, label = preds.mean(dim=1), label.mean(dim=1)\n",
    "                correctness_list.append((preds*label>0).float().mean())\n",
    "                \n",
    "                total_valid_loss += loss.item()\n",
    "\n",
    "        avg_valid_loss = total_valid_loss / count\n",
    "        \n",
    "        test_acc = np.array([t.cpu().numpy() for t in correctness_list]).mean()\n",
    "        print(f'Test acc : {test_acc}')\n",
    "\n",
    "        # **åˆ—å°è¨“ç·´ & é©—è­‰æå¤±**\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_valid_loss:.4f}\")\n",
    "        \n",
    "\n",
    "train_single_day()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca7e2c-f0f6-4b1e-84c6-6180e28851b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267bba5-9fc7-46ef-9d55-887e9c80ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data = yf.download('AAPL', start='2014-01-01', end='2025-02-01', progress=False)\n",
    "historical_data[\"daily_return\"] = historical_data[\"Adj Close\"].pct_change()\n",
    "        \n",
    "historical_return = historical_data['daily_return']\n",
    "        date = self.trading_days[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba00ae-bfaf-45ae-bfcf-fa1e41fb1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "A = torch.tensor([1,2,3,4,5]).unsqueeze(1)\n",
    "B = torch.tensor([-1,-2,1,9,9]).unsqueeze(1)\n",
    "(A*B>0).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb095017-64da-4fc1-9106-adef732bc44b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "finance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
